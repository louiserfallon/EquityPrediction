---
title: "Big Data In Finance Individual Assignment"
author: "Louise Fallon"
output: pdf_document
---

```{r setup, include=FALSE}
library(readxl) #for read_excel
library(ggplot2) #for ggplot
library(reshape2) #for melt
library(condformat) #for condformat and rule_fill_discrete
library(knitr) #for kable
library(gridExtra) #for gridarrange
knitr::opts_chunk$set(cache = FALSE, echo=FALSE)
```

## Introduction & Data

```{r loaddata}
#load data
returns <- as.data.frame(read_excel("Data.xlsx", sheet = 1, col_names = TRUE, skip=3))
#remove lagged spaces in column names
for (i in 1:ncol(returns)) colnames(returns)[i] <- gsub(" ","",colnames(returns)[i])
#remove row of NAs at the bottom
returns <- returns[complete.cases(returns),]
#add date column header
names(returns)[1] <- "date"
#plot boxplot of returns
meltedreturns <- melt(returns, id.vars = "date")
```
Using daily data on value-weighted returns from 49 industry-based portfolios, multiple algorithms are trained and compared to predict future returns for each industry. The daily industry returns shown below are generally centered around 0, the lowest average return is for Coal at `r format(min(colMeans(returns[,2:50])),digits=3)`, and the highest is for Gold at `r format(max(colMeans(returns[,2:50])),digits=3)`, with clear differences in industry return variance ranging from 
`r format(min(apply(returns[,2:50], 2, var)),digits=3)` for Household, and `r format(max(apply(returns[,2:50], 2, var)),digits=3)` for Coal. For this data to build a useful predictive model, we require a relationship between industry returns and lagged returns either in the same industry, or across other industries.

```{r boxplot, fig.height=2.5, fig.align='center'}
ggplot(meltedreturns, aes(x=variable, y=value)) +
  geom_boxplot(col="#000066") + 
  theme_bw() + 
  theme(legend.position="none",
        axis.text.x = element_text(angle = 90, vjust=0.5, size=6)) +
  xlab("Industry") + ylab("Daily Returns")
```

If industry momentum is a real phenomenon, as per (Grinblatt and Moskowitz 1999), where buying industry portfolios that are performing well and selling those not performing well is a money-making strategy, then we may expect to see a correlation between current returns with previous returns, within an industry. One of the reasons why we may expect to see momentum strategies performing well, as explained by (Hong and Stein 1999), is when information slowly diffuses through the network of investors, prices underreact to news in the short term as not everyone has heard the news, and overreact in the long term due to momentum traders. This is something we may expect to see at the daily return level. Momentum may still exist without autocorrelation (Lewellen 2002), but if past performance does have a relationship with future performance, either positively or negatively, but at least consistently, then including lagged variables of own-industry returns would be beneficial for a predictive model.

Additionally, if there are reliable relationships between one industry's current returns and lagged returns of other industries in the dataset, then this would also aid predictive model building, non-linear models could also make use of any interactions between industry and lag returns combinations. We might expect to see positive cross-correlations on occasions where there is an upstream/downstream industry relationship e.g. with Textiles and Clothes, where information is gradually diffused (Menzly and Obas 2010), or where there are differences in analyst coverage (Brennan, Jegadeesh and Swaminathan 1993) such that industries with high levels of average coverage "lead" industies with lower levels. 

##Cross-correlation

```{r corrplotcalc}
corrmatrix <- matrix(0, ncol=49,nrow=49)
plotlist <- list()
for (lag in 1:6){
  for (i in 1:49){
    for (j in 1:49){
#correlating the current value of i, with the lagged value of j
#starting from second period so that lagged values can be calculated
#the +1 is to avoid the date column
    corrmatrix[i,j] <- round(cor(returns[(lag+1):nrow(returns),i+1],
                                 returns[1:(nrow(returns)-lag),j+1]), 2)
    }
  }
melteddf<- melt(corrmatrix)
plotlist[[lag]] <- ggplot(data = melteddf, aes(x = Var1, y = Var2, fill = value)) + 
    geom_tile() + 
    labs(x = "", y = "", title = "") +
    theme_bw() + ylab("Industry returns at time t") + xlab(paste("Industry returns at time t-",lag,sep="")) + theme_void() +
     theme(axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.x = element_blank(),
          axis.ticks.y = element_blank(),
          axis.title=element_text(size=8, color="#888888"),
          panel.border = element_blank(),
          legend.position = 'none') + 
  scale_fill_gradient2(low = "#ce1254",mid="#ffffff", high = "#006699",
                       midpoint = 0, limit = c(-.2,.2), space = "Lab",
                       name="Pearson\nCorrelation") 
}

corrtest <- matrix (0, nrow=49, ncol=49)
##recreate correlation matrix for lag 4, including test
for (i in 1:49){
  for (j in 1:49){
    corrmatrix[i,j] <- round(cor(returns[5:nrow(returns),i+1],
                                 returns[1:(nrow(returns)-4),j+1]), 2)
    corrtest[i,j] <- cor.test(returns[5:nrow(returns),i+1],
                                 returns[1:(nrow(returns)-4),j+1])$p.value
  }
}

g_legend<-function(a.gplot){ 
  tmp <- ggplot_gtable(ggplot_build(a.gplot)) 
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box") 
  legend <- tmp$grobs[[leg]] 
  return(legend)}

plotforlegend <-  ggplot(data = melteddf, aes(x = Var1, y = Var2, fill = value)) + 
    geom_tile() + 
  theme(legend.title=element_text(size=8, color="#888888"),
        legend.text = element_text(colour = "#888888", size=6)) +
  scale_fill_gradient2(low = "#ce1254",mid="#ffffff", high = "#006699",
                       midpoint = 0, limit = c(-.2,.2), space = "Lab",
                       name="Pearson\nCorrelation")                                      
legend <- g_legend(plotforlegend)
```

In almost all cases the contemporaneous correlations are found to be positive, which indicates that the overall market moves in a similar direction, potentially according to business cycle trends. The only clear anomaly in this case is Gold, which has much lower correlation, and in some cases a slighty negative correlation against Clothes and Banks, which is likely due to the fact that gold is often used as a hedge against stocks, and gold companies also carry this effect (Baur and Lucey 2010). This contemporaneous information is not particularly useful for the predictive task at hand, as prediction will be made for a time period in the future, when returns of other industries in that future time period will not be known.

The plots below show the correlations between the industry returns at time t, against their returns at various lags. At lags of 2 and 4 there is a clear pattern of negative cross correlations, except for some outlier industries (these include Gold, Coal, Oil and interestingly for lag 4 also Fun). This negative cross-correlation is not in line with the results from (Lo and MacKinlay 1990), although they use a different time period. The effect we could be seeing here is a "balancing" effect where short-term investors, potentially following a momentum strategy, move money out of poorly performing industries and reinvest that money in other industries within a time frame of 2 or 4 days, pushing up the price in those industries and vice versa.

Regardless of the sign, if these correlations are significant then they are expected to be useful within the predictive models, which for a 4-lag `r length(corrtest[corrtest<0.1])` are at the 10% significance level, and `r length(corrtest[corrtest<0.05])` are at the 5% significance level. Although these correlations are low in absolute value, and this is only a percentage of the `r 49*49` possible combinations, prediction of equity returns with previous returns is a situation with a low signal:noise ratio, and algorithms that deal with scarcity can potentially use these correlations to aid prediction e.g. the LASSO method (Chinco 2017).

```{r corrplot, fig.height=3.5}
lay <- matrix(c(1,2,3,
                4,5,6),ncol=3,byrow = TRUE)
grid.arrange(plotlist[[1]],plotlist[[2]],plotlist[[3]],plotlist[[4]],plotlist[[5]], legend, layout_matrix= lay)
```

##Auto-correlation

```{r boxpierce, cache=FALSE}
boxpiercevalue <- matrix(0, nrow = 49, ncol = 10) 
autocorrelations <- matrix(0, nrow = 49, ncol = 10) 
for (i in 1:49){
  for (j in 1:10){
    boxpiercevalue[i,j] <- round(Box.test(ts(returns[,i+1],
                                       start=c(1,20),
                                       frequency=252), lag=j)$p.value,3)
    autocorrelations[i,j] <- round(cor(returns[(1+j):nrow(returns),i+1],
                                       returns[1:(nrow(returns)-j),i+1]),3)
  }
}

autocorrelationsdf <- as.data.frame(autocorrelations)
autocorrelationsdf <- cbind(names(returns)[2:50],autocorrelationsdf)
names(autocorrelationsdf) <- c("Industry","Lag 1","Lag 2","Lag 3",
                        "Lag 4","Lag 5","Lag 6","Lag 7",
                        "Lag 8","Lag 9","Lag 10")

posautocorrelations <- vector()
negautocorrelations <- vector()

pval <- 0.1
for (i in 1:10)
{posautocorrelations[i] <- length(boxpiercevalue[,i][boxpiercevalue[,i] < pval & autocorrelationsdf[,i+1] > 0])
 negautocorrelations[i] <- length(boxpiercevalue[,i][boxpiercevalue[,i] < pval & autocorrelationsdf[,i+1] < 0])
}
dfforoutput <- as.data.frame(rbind(posautocorrelations,negautocorrelations))
rownames(dfforoutput) <- NULL
dfforoutput <- cbind(c("Positive","Negative"),dfforoutput)
colnames(dfforoutput) <- c("",paste("lag_",1:10,sep=""))
kable(dfforoutput)
```

Using a Box-Pierce test each industry return series was tested for autocorrelation with lags up to 10 working days. The results displayed above show the number of cases (industries) where the test provided evidence of autocorrelation at the 10% significant level. This has been split into cases where the original correlation was positive or negative. In most cases there is no statistically significant autocorrelation for an industry portfolio, but in cases where there is autocorrelation, this is often negative. There also seems to be a pattern where a lag of 4 has some evidence of negative relationships with returns, both within industry and across industries. The models built in the next section therefore use lags up to 4 as input variables.

#Model Building

Models were built to predict at time $\tau$, using data known at time $\tau$, the value of industry returns at time $\tau + 1$, using a rolling training window of 80 days to identify out of sample performance as per the model of (Goyal and Welch 2008).

The first model is a pure linear Ordinary Least Squares (OLS) model of returns as a function of the previous day's returns within the industry [Lin Own 1], of the form: $y_{i,t} = \beta_0 + \beta_1 y_{i,t-1}$, where y refers to the returns, i refers to the industry and t refers to the time period (working day). The second is a linear model of returns a function of the 4 previous day's returns within the industry [Lin Own 4], of the form  $y_{i,t} = \beta_0 + \beta_1 y_{i,t-1} + \beta_2 y_{i,t-2} + \beta_3 y_{i,t-3} + \beta_4 y_{i,t-4}$. The third is a LASSO model of returns as a function of the 4 previous day's returns within the industry [L Own 4], this has the same functional form as the 4-own-lag linear model, but improves on it by penalising the absolute values of the coefficients, this has the effect of removing the coefficients on the lags that are not relevant within the time period, or that are in a group of multicollinear variables, when only the most predictive variable is chosen (Chinco 2017), this penalisation parameter is chosen via cross-validation within each window. The LASSO also scales input variables.

The fourth is a linear model of returns as a function of the previous day's returns from all industries [Lin All 1], of the form: $y_{i,t} = \beta_0 + \beta_1 y_{i,t-1} + \sum\limits_{k \neq i \in K} \beta_k x_{k,t-1}$, where K is the set of industries. This is likely to overfit to the training data, as there will be 49 industry returns that are maximally only slightly correlated to the target industry returns. The fifth is a LASSO model of returns as a function of the previous day's returns from all industries, this has the same functional form as the above model but it is expected to outperform as it is able to "choose"  the most relevant variables [L 1].

The sixth is a LASSO model using 4 lags from all industries [L 4], of the form: $y_t = \beta_0 + \sum\limits_{k \in K} \beta_{k,1} x_{k,t-1} + \sum\limits_{k \in K} \beta_{k,2} x_{k,t-2} + \sum\limits_{k \in K} \beta_{k,3} x_{k,t-3} + \sum\limits_{k \in K} \beta_{k,4} x_{k,t-4}$. In this form the linear model would not be possible within a window of 80 as there would be more variables than data points and the system would be undetermined. It is expected that this will outperform the other models as it includes the 4-lag variables which were seen to have some statistically significant auto-correlations and cross-correlations.

The seventh is LASSO model using 4 lags from all industries, and also including an indicator variable of whether there was at least one front page news article from the social media news aggregation platform Reddit containing the words of one of the major companies within that industry [L 4 news], of the form $y_t = \beta_0 + \sum\limits_{k \in K}  \beta_{k,1} x_{k,t-1} + \sum\limits_{k \in K}  \beta_{k,2} x_{k,t-2} + + \sum\limits_{k \in K} \beta_{k,3} x_{k,t-3} + \sum\limits_{k \in K} \beta_{k,4} x_{k,t-4} + \sum\limits_{h \in H} \delta_{h,news} x_{h,news}$ . This is follwing on from many pieces of analysis that attempt to include news sources in financial prediction, e.g. (Hagenau 2013), to see if this can improve performance in this case, only industries where any news stories were identifiable were included, hence the h notation, where H $\subseteq$ K.

The eighth is a random forest model based on the 4 lagged data, which is an ensemble of classification trees built up using different subsets of the independent variables, it allows for non-linear combinations of data [RF 4], the number of trees and the minimum number of samples in a leaf are both tuned via in-window cross validation. The ninth is a generalised boosting regression tree technique, another ensembling technique where regression trees are added iteratively, but using the optimal gradient to reduce error [GBM 4], here there is a shrinkage rate applied to limit overfitting which is also tuned via in-window cross validation.

The results of these models are detailed in the table below, with the $R^2_{oos}$ as calculated in (Goyal and Welch 2008) as $1-\frac{\sum_{t=1}^{t=T}(r_t-\hat{r_t})^2}{\sum_{t=1}^{t=T}(r_t-\bar{r_t})^2}$. Which is 1 minus the ratio between the sum of squared errors from the prediction and the sum of squared errors from the historical mean. These refer to the prediction and the mean calculated at time $\tau$ for time $\tau+1$, so reflect an out of sample (oos) estimate. A positive $R^2_{oos}$ indicates that across this time window, the model beats the historical mean. The $R^2_{oos}$ is shown for each industry, and the average across all industries. Positive, non-zero $R^2_{oos}$ values are highlighted.

\newpage
```{r R2oos}
load("R2oos.Rdata")
load("condR2table.RData")
condr2table
```

#Model Performance

Firstly, as expected, these models do not perform particularly well in comparison to the historical mean approach. Using previous equity returns does not help explain much of the variance in future equity returns, and therefore are not particularly useful for prediction. However some models do perform better than others. 

As expected, [Lin 1] performs worse than [Lin Own 4], which performs worse than [Lin Own 1], this is likely due to model overfitting.  When the model is trained within a training window, the coefficients will be estimated and essentially matched to the training data. As the structural form of the model grows, there are more coefficients that will be estimated, but these may not reflect actual economic relationships, and the model will thererefore perform worse out of sample. In this case it is unlikely that there are actual, non-overlapping relationships with the industry returns and all of the independent variables (either with all 4 lags, or with all other industries with 1 lag), and so the parameter estimation causes this overfitting problem.

When using the LASSO approach which penalises parameters, this overfitting is somewhat accounted for, and a better result is achieved, with a positive $R^2_{oos}$ for 7 industries using [L own 4], for 11 industries with [L 1], and for 14 industries with [L 4]. This is because the LASSO approach "selects" parameters to be included based on their relevance at the time, and is therefore able to identify sparse useful economic signals (Chinco 2017). The fact that the LASSO method does not necessarily estimate a coefficient for each variable means that it is able to overcome overfitting issues that the linear model is unable to. For [L own 4] the cross-validation approach often chooses a penalisation parameter that left no non-zero coefficients, and is therefore equivalent to the historical mean, so although this has the highest average $R^2_{oos}$, it is not particularly impressive as it rarely deviates from a very naive strategy. The [L 4] model is probably the most useful, particularly for some industries e.g. Construction, where it shows a consistent improvement over time, shown below using cumulative squared error compared to the historical mean:

```{r SEdiffcalc}
load("SEforGoyal.RData")
lasso.cv.4.allSEdiff <- lasso.cv.4.allSE - historicalmean.4SE
colCumSums <- function(x) {
  for(i in seq_len(dim(x)[2])) { x[,i] <- cumsum(x[,i]) }; x
}
lasso.cv.4.allSEdiffcum <- colCumSums(lasso.cv.4.allSEdiff)
lasso.cv.4.allSEdiffcum <- cbind(returns$date[84:nrow(returns)],lasso.cv.4.allSEdiffcum)
colnames(lasso.cv.4.allSEdiffcum) <- c("date",colnames(returns)[2:ncol(returns)])
lasso.cv.4.allSEdiffcum <- as.data.frame(lasso.cv.4.allSEdiffcum)
lasso.cv.4.allSEdiffcum$date <- as.Date(as.character(lasso.cv.4.allSEdiffcum$date),"%Y%m%d")
```

```{r goyalplot, eval=FALSE}
dfforgoyalplot <- melt(lasso.cv.4.allSEdiffcum, id.vars="date")
ggplot(dfforgoyalplot, aes(x=date,y=value, col=variable)) + geom_line(alpha=0.8) + theme_light() + ggtitle("") + ylab("L4 Cumulative SE difference") + xlab("") + geom_hline(yintercept=0, color="#555555") + theme(legend.text=element_text(size=4)) + labs(color='') 
```

```{r out.width='60%', fig.align='center'}
#included as a graphic so that the legend was included properly
knitr::include_graphics("L4cumSEdiff.png")
```


The addition of news information from Reddit did not improve accuracy using the LASSO [L 4 news], this could be because the economic signals they may indicate are encorporated into returns on the same day, so they do not add value for predicting tomorrow's returns. It could also be due to the sparsity of financial news in this dataset, the source was a public social network, Reddit, and there was often fewer than two pieces of news within an 80 day period, so the rolling window models were unable to train to pick up on these signals.  A more financial news website such as Reuters may have been more dense and also more predictive.

Non linear methods were also not particularly effective, the random forest [RF 4] which allows multiple non-linear relationships to be estimated and averaged, does not lead to a positive $R^2_{oos}$ in any industry, this is likely due to the sparsity of the signals and the fact that a relatively small window (80) is being used, the random forest would be expected to perform more effectively with more input data. The gaussian generalised boosting method [GBM 4] is slightly better, it has a more intelligent mechanism of growing new trees using the gaussian gradient which minimises squared error, but the sparsity of signals and the small window size are, as with the random forest, likely key factors in why this method does not outperform the LASSO.

\newpage
```{r coeftable}
load("coefcounttable.Rdata")
kable(coefcounttable)
```

##Parameter Inclusion

The table above shows, for [L 4], the number of windows for which the trained model had a coefficient that was not equal to 0. It shows this for each of own-industry lags, and then shows the other top 2 most used parameters. This is out of a total of 441 windows. The table shows that the signals picked up are very sparse, with the most frequent being 29, the Clothes industry 1-lag predicting Real Estate returns, which is not a combination that is particularly intuitive. We do not see many expected relationships, the LASSO uses the historical mean for every window for the Textiles Industy, and the Clothes industry does not have Textiles as a predictor. It is interesting that the Health, Drugs and Clothes industries are relatively prominent across various lags. This could potentially be that these industries have larger companies within them, and we are seeing the lag-lead effects from (Lo & MacKinlay 1990), but this is not a very convincing argument, and it does seem that the results are not overly sensible in an economic sense. ^[Note that these results are after returns have been scaled, without which the more variable inputs, e.g. Gold and Oil, would be more heavily represented.]

##Conclusion
The models are generally not effective at predicting future equity returns consistently across time periods and industries. This is aligned with the weak-form efficient market hypothesis that future prices (and therefore returns) cannot be predicted by past prices. The LASSO model does seem to outperform other models, particularly for some industries, by identifying sparse signals across industries and lags, yet avoiding overfitting. Additional information would likely be useful in improving the predictive performance of these models, for example industry-specific press releases, or financial data of the companies within the portfolios.

##References

Baur, D.G., Lucey, B. M., (2010) Is Gold a Hedge or a Safe Haven? An Analysis of Stocks, Bonds and Gold *The Financial Review* 45(2), 217–229.

Brennan, M., Jegadeesh, N., Swaminathan, B. (1993) Investment Analysis and the Adjustment of Stock Prices to Common Information *Review of Financial Studies* 6(4), 799-824.

Chinco, A., Clark-Joseph, A., and M. Ye, (2017) Sparse Signals in the Cross-Section of Returns, Working paper. Accessible at: http://www.alexchinco.com/sparse-signals-in-cross-section.pdf [Accessed 10th April 2017]

DeMiguel, V., Nogales F. J, Uppal, R. (2014) Stock Return Serial Dependence and
Out-of-Sample Portfolio Performance, *Review of Financial Studies* 27 (4), 1031–
1073.

Goyal, A., Welch, I. (2007) A Comprehensive Look at the Empirical Performance of Equity Premium Prediction. *Review of Financial Studies, Oxford University Press for Society for Financial Studies* 21(4), 1455-1508. 

Hagenau. M, Leibemann, M. Neumann. D. (2013) Automated news reading: Stock price prediction based on financial news using context-capturing features. *Decision Support Systems* 55(3), 685-697.

Lewellen, J. (2002) Momentum and Autocorrelation in Stock Returns. *The Review of Financial Studies* 15(2), 533-563

Lo, A., MacKinlay A. C. (1990) When are contrarian profits due to stock market overreaction? *Review of Financial Studies* 3(2), 175–205.

Menzly, L., Ozbas O. (2010) Market segmentation and cross-predictability of returns. *Journal of Finance* 65, 1555–80.

Moskowitz, T., Grinblatt M (1999) Do Industries Explain Momentum? *The Journal of Finance* 54 (4), 1249-1290. 